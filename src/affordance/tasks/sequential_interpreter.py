import ast
import pdb
import re
import subprocess
import sys
import time

import enchant
import openai
from pot_tools import safe_execute, simplify_ans
import requests
from tqdm import tqdm
from utils import Command, OpenAIModel, StacktraceItem, gpt3, parse_program

subscription_key = "28dc412935f24fb9974d80c30915483a"
search_url = "https://api.bing.microsoft.com/v7.0/search"
headers = {"Ocp-Apim-Subscription-Key": subscription_key}
import time

from serpapi import GoogleSearch

serp_api_key = "5599fad2263e4b1c6d4efb62fa15f83f5ad2d1bb727b8721d616d719399a7f7b"


import re

# as per recommendation from @freylis, compile once only
CLEANR = re.compile("<.*?>")


def cleanhtml(raw_html):
    cleantext = re.sub(CLEANR, "", raw_html)
    return cleantext


def search(query, top_k=1):
    params = {"q": query, "textDecorations": True, "textFormat": "HTML"}
    response = requests.get(search_url, headers=headers, params=params)
    response.raise_for_status()
    search_results = response.json()
    if "webPages" in search_results:
        snippets = [
            cleanhtml(v["name"] + " " + v["snippet"]) for v in search_results["webPages"]["value"]
        ][:top_k]
    else:
        return "", None
    return "\n".join(snippets), None


def google_search(query, previous_input=None, top_k=1):
    params = {
        "q": query,
        #   "location": "Austin, Texas, United States",
        "hl": "en",
        "gl": "us",
        "google_domain": "google.com",
        "api_key": serp_api_key,
    }

    search = GoogleSearch(params)
    try:
        res = search.get_dict()
    except:
        res = {}

    if "answer_box" in res.keys() and "answer" in res["answer_box"].keys():
        toret = res["answer_box"]["answer"]
    elif "answer_box" in res.keys() and "snippet" in res["answer_box"].keys():
        toret = res["answer_box"]["snippet"]
    elif "answer_box" in res.keys() and "snippet_highlighted_words" in res["answer_box"].keys():
        toret = res["answer_box"]["snippet_highlighted_words"][0]
    elif "organic_results" in res.keys() and "snippet" in res["organic_results"][0].keys():
        toret = res["organic_results"][0]["snippet"]
    else:
        toret = None

    return toret, None


def code_generate(instruction, code_input):
    # response = openai.Edit.create(
    # model="code-davinci-edit-001",
    # input="x = " + code_input,
    # instruction="Python code for " + instruction,
    # temperature=0,
    # top_p=1
    # )
    if instruction in code_input:
        # Code input is sufficient to write code
        comment = '"""\n{0}\nStore the final result as a variable named \'ans\' and print it.\n"""\n\n'.format(
            code_input
        )
    else:
        if re.search("#[0-9]+", instruction):
            input_string = re.search("#[0-9]+", instruction).group(0)
            instruction = instruction.replace(input_string, "Input")
        # comment = "\"\"\"\n{0}\nInput:{1}\nStore the final result as a variable named 'ans' and print it.\n\"\"\"\n\ndef".format(instruction, code_input)
        comment = '"""\n{0}\nInput:{1}\nStore the final result as a variable named \'ans\' and print it.\n"""\n\n'.format(
            instruction, code_input
        )

    response = openai.Completion.create(
        # model="code-davinci-002",
        model="davinci-codex-002-msft",
        prompt=comment,
        temperature=0.0,
        max_tokens=256,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
        stop=["print(ans)"],
    )

    code_snippet = response.choices[0].text
    time.sleep(5)

    # TODO: Add function run or remove the "def" entirely
    # return "def" + code_snippet, comment
    # if "ans" in code_snippet:
    #     code_snippet += "\nprint(ans)"

    # TODO: Add "print(ans) to the end."
    return code_snippet, comment


def code_execute(code_snippet, code_input=None):
    if code_input:
        try:
            # Run arithmetic python code generate
            code_output = safe_execute(code_input)
            result = simplify_ans(code_output)
            if result:
                return result, code_input
        except:
            pass

    try:
        # Run code snippet as is, if it was generated by code_generate
        # More robust outputs
        result = subprocess.run(
            [sys.executable, "-c", code_input],
            capture_output=True,
            text=True,
            timeout=60,
        )
        if result.stderr == "":
            simplified_result = simplify_ans(result.stdout)
            return simplified_result, code_snippet
    except:
        pass

    try:
        # Run arithmetic python code generate
        code_output = safe_execute(code_snippet)
        result = simplify_ans(code_output)
        if result:
            return result, code_snippet
    except:
        pass

    try:
        # Run code snippet as is, if it was generated by code_generate
        result = subprocess.run(
            [sys.executable, "-c", code_snippet],
            capture_output=True,
            text=True,
            timeout=60,
        )
        if result.stderr == "":
            simplified_result = simplify_ans(result.stdout)
            return simplified_result, code_snippet
    except:
        pass

    if "error" in code_snippet or (code_input and "error" in code_input):
        return code_error(code_snippet, code_input)

    try:
        # Command instruction (if its code) starts in the next line.
        code_snippet = code_snippet.split("\n", 1)[1]
        result = subprocess.run(
            [sys.executable, "-c", code_snippet],
            capture_output=True,
            text=True,
            timeout=60,
        )
        if result.stderr == "":
            simplified_result = simplify_ans(result.stdout)
            return simplified_result, code_snippet
    except:
        return None, code_snippet

    return None, code_snippet


def code_generate_then_execute(instruction, code_input):
    # output = openai.Edit.create(
    # model="code-davinci-edit-001",
    # input="x = " + code_input,
    # instruction="Python code for " + instruction,
    # temperature=0,
    # top_p=1
    # )
    # code_snippet = output.choices[0].text

    code_snippet, generate_details = code_generate(instruction=instruction, code_input=code_input)
    time.sleep(2)
    # result = subprocess.run([sys.executable, "-c", code_snippet], capture_output=True, text=True)
    # if result.stderr == "":
    #     return result.stdout, code_snippet
    result = code_execute(code_snippet=code_snippet)
    return result


def code_error(code_snippet, code_input):
    # Command instruction (if its code) starts in the next line.
    try:
        code_snippet = code_snippet.split("\n", 1)[1]
        result = subprocess.run(
            [sys.executable, "-c", code_snippet],
            capture_output=True,
            text=True,
            timeout=60,
        )
        if result.stderr != "":
            return result.stderr, code_snippet
    except:
        return None, code_snippet


def code_edit(instruction, code_input):
    comment = "# " + instruction + "\n"
    response = openai.Completion.create(
        # model="code-davinci-002",
        model="davinci-codex-002-msft",
        prompt=comment,
        temperature=0.0,
        max_tokens=256,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
        stop=["\n"],
    )

    code_snippet = response.choices[0].text
    return comment + code_snippet, comment


def code_generate_then_lookup(instruction, code_input):
    code_snippet, generate_details = code_generate(instruction=instruction, code_input=code_input)
    time.sleep(2)
    result, execute_snippet = code_execute(code_snippet=code_snippet)
    d = enchant.Dict("en_US")
    try:
        word_list = ["".join(chat_list) for chat_list in ast.literal_eval(result)]
        valid_list = []
        for word in word_list:
            if d.check(word):
                valid_list.append(word)
        return simplify_ans(valid_list), execute_snippet
    except:
        return None, execute_snippet


def lookup(word_list):
    import enchant

    d = enchant.Dict("en_US")
    valid_list = []
    for word in word_list:
        if d.check(word):
            valid_list.append(word)
    return valid_list


def arithmetic(equations, previous_input):
    # collect all outputs of arithmetic and run python to execute them
    result = subprocess.run(
        [sys.executable, "-c", "\n".join(equations)], capture_output=True, text=True
    )
    return result, None


def generate(prompt, previous_input):
    return gpt3(prompt)[0], None


class Interpreter:
    def __init__():
        pass


class TopDownVisitor(Interpreter):
    def __init__(self, model_name="text-davinci-002"):
        self.built_ins = {
            "[generate]": generate,
            "[search]": search,
            "[code generate]": code_generate,
            "[code execute]": code_execute,
            "[string edit]": code_generate_then_execute,
            # "[arithmetic]" : arithmetic,
            "[generate python code]": code_generate,
        }

        self.program_completer = OpenAIModel(model=model_name, max_length=500, quote="", n=1)

    def syntax_check(self, program):
        # Look for programs with EOC ending and final answer in syntax.
        return "[EOQ]\nAns:" in program

    def batch_visit(self, prefixes, programs):
        answers = []
        for prefix, program in tqdm(zip(prefixes, programs)):
            answers.append(self.visit(prefix, program))
        return answers

    def check_builtin(self, command):
        # Some multiple commands are mapped to the same built-in affordances.
        processed_command = command
        if command in self.built_ins:
            return self.built_ins[processed_command]
        # What to do when no built-infunction is triggered.

    def complete_program(self, prefix, program):
        # Check for out-of-window errors and shift window to reduce the number of programs shown.
        continuation = program
        runs = 5
        while not self.syntax_check(continuation) and runs > 0:
            continuation = self.program_completer(prefix + program)[0]
            runs -= 1
        return program + continuation

    def rerun_program(self, prefix, prev_command_list, current_command_output):
        # Check for out-of-window errors and shift window to reduce the number of programs shown.
        program_prefix = prefix + "\n".join(
            [
                Command.convert_to_nlprogram(i + 1, command)
                for i, command in enumerate(prev_command_list[:-1])
            ]
        )
        # TODO: Add feature for multiline
        program_prefix += (
            "\n"
            + Command.convert_to_nlprogram(
                len(prev_command_list), prev_command_list[-1], input_only=True
            )
            + "\n#{0}: {1}".format(len(prev_command_list), current_command_output)
        )
        # Replace last line with corrected input, if necessary.
        continuation = self.program_completer(program_prefix)[0]
        return program_prefix, continuation

    def visit(self, prefix, program):
        program_ends = self.syntax_check(program)
        program = prefix.rsplit("----\n", 1)[1].split("\n", 1)[1] + program
        # If program does not end correctly, rerun GPT-3 further to end it.
        if program_ends:
            parsed_program = parse_program(program)
        else:
            program = self.complete_program(prefix, program)
            program_ends = self.syntax_check(program)
            if not program_ends:
                # After one attempt at completing program, give up checking for affordance
                return program
            parsed_program = parse_program(program)

        previous_input = None
        i = 0
        parsed_program.input_node
        command_node_list = parsed_program.node_list
        parsed_program.answer_node
        stack_trace = []
        while command_node_list[i].command_name != "[EOQ]":
            node = command_node_list[i]
            command_name = node.command_name
            command_input = node.command_input
            command_output = node.command_output
            # Check affordance list to run.
            affordance = self.check_builtin(command=command_name)

            if (
                affordance and previous_input
            ):  # previous_input so code generate is hard, But search and code_execute is not, overrride.
                affordance_output, affordance_details = affordance(command_input, previous_input)
                if affordance_output:
                    # affordance_output and command_output (i.e. GPT-3) need to be interpolated appropriately.
                    # For code executions
                    command_output = affordance_output
                    # For Search (concatenate the GPT_3 output with retrieval output)

                # Run GPT-3 again.
                program_prefix, rerun_program = self.rerun_program(
                    prefix, command_node_list[: i + 1], command_output
                )

                # Replace nodes i+1 though N (again complication is that this should be done EOC i.e. till the program is fully generated.)
                new_program = program_prefix + rerun_program
                new_program = new_program.rsplit("----\n", 1)[1].split("\n", 1)[1]
                try:
                    parsed_rerun_program = parse_program(new_program)
                except:
                    pdb.set_trace()

                parsed_program = parsed_rerun_program
                program = new_program

                # for j in range(i+1, len(command_node_list)):
                #     command_node_list[j] = parsed_rerun_program.node_list[j]
                command_node_list = (
                    command_node_list[: i + 1] + parsed_rerun_program.node_list[i + 1 :]
                )

                parsed_rerun_program.answer_node

                stack_trace.append(
                    StacktraceItem(
                        node,
                        affordance,
                        affordance_output,
                        affordance_details,
                        new_program,
                        parsed_rerun_program,
                    )
                )

            # Output of the current command becomes the input for the next round.
            previous_input = command_output
            i += 1

        # Since run is complete, The final answer in parsed_rerun_program is returned
        # Selectively, we can also return the entire program
        return program


class TopDownVisitorBeta(Interpreter):
    def __init__(
        self,
        model_name="text-davinci-002",
        temperature=0.3,
        rerun=True,
        exclude_list=[],
    ):
        self.built_ins = {
            "[generate]": generate,
            "[search]": google_search,
            "[code generate]": code_generate,
            "[code execute]": code_execute,
            "[code edit]": code_edit,
            "[string edit]": code_generate_then_execute,
            "[string index]": code_generate_then_execute,
            "[string permutation]": code_generate_then_lookup,
            # "[arithmetic]" : arithmetic,
            "[generate python code]": code_generate,
        }
        if len(exclude_list):
            for key in exclude_list:
                del self.built_ins[key]
        self.keyword_map = {
            "[permutation]": "[string permutation]",
            "[permute]": "[string permutation]",
            "[string permute]": "[string permutation]",
            "[execute]": "[code execute]",
        }

        self.execution_details = []
        self.rerun = rerun
        self.program_completer = OpenAIModel(
            model=model_name, temperature=temperature, max_length=500, quote="---", n=1
        )

    def syntax_check(self, program):
        # Look for programs with EOC ending and final answer in syntax.
        return "[EOQ]\nAns:" in program

    def batch_visit(self, prefixes, programs):
        answers = []
        for prefix, program in tqdm(zip(prefixes, programs)):
            answers.append(self.visit(prefix, program))
        return answers

    def check_builtin(self, command):
        # Some multiple commands are mapped to the same built-in affordances.
        processed_command = command
        if command in self.built_ins:
            return self.built_ins[processed_command]
        elif command in self.keyword_map:
            return self.built_ins[self.keyword_map[processed_command]]
        # What to do when no built-infunction is triggered.

    def shorten_prefix(self, prefix, to_remove=1):
        # Program prefix contains other demonstrations, along with Q1:
        individual_programs = prefix.split("\n----\n")

        # to Remove: Number of programs to remove
        # If they are organized by task relevance probably best to get rif of the latter ones.
        shifted_programs = individual_programs[: -(to_remove + 1)] + [individual_programs[-1]]
        new_prefix = "\n----\n".join(shifted_programs)

        return new_prefix

    def complete_program(self, prefix, program):
        # Check for out-of-window errors and shift window to reduce the number of programs shown.
        continuation = program
        runs = 5
        prefix = self.shorten_prefix(prefix, 1)
        while not self.syntax_check(continuation) and runs > 0:
            # TODO: Explicitly, check if the program ends in answers or questions and accordingly change the separator.
            separator = "\nQ"
            continuation += separator
            continuation = self.program_completer(prefix + continuation)[0]
            continuation = program + separator + continuation
            runs -= 1
            prefix = self.shorten_prefix(prefix, 1)
        return continuation

    def rerun_program(self, prefix, prev_command_list, current_command_output):
        # Check for out-of-window errors and shift window to reduce the number of programs shown.
        program_prefix = prefix + "\n".join(
            [
                Command.convert_to_nlprogram(i + 1, command)
                for i, command in enumerate(prev_command_list[:-1])
            ]
        )
        # TODO: Add feature for multiline
        output_sep = "\n" if "\n" in current_command_output else " "
        program_sep = "" if len(prev_command_list) == 1 else "\n"
        program_prefix += (
            program_sep
            + Command.convert_to_nlprogram(
                len(prev_command_list), prev_command_list[-1], input_only=True
            )
            + "\n#{0}:{1}{2}\nQ".format(
                len(prev_command_list), output_sep, current_command_output.strip()
            )
        )
        # Replace last line with corrected input, if necessary.
        continuation = self.program_completer(program_prefix)[0]
        return program_prefix, continuation

    def rerun_answer(self, prefix):
        continuation = self.program_completer(prefix)[0]
        return continuation

    def visit(self, prefix, program):
        program_ends = self.syntax_check(program)
        program = prefix.rsplit("----\n", 1)[1].split("\n", 1)[1] + program
        # If program does not end correctly, rerun GPT-3 further to end it.
        if program_ends:
            try:
                parsed_program = parse_program(program)
            except:
                # If the initial program is not parsed even if it has an ending, return as is
                return program
        else:
            new_program = self.complete_program(prefix, program)
            program_ends = self.syntax_check(new_program)
            if not program_ends:
                # After one attempt at completing program, give up checking for affordance
                return program
            try:
                parsed_program = parse_program(new_program)
            except:
                return program

        previous_input = None
        i = 0
        input_node = parsed_program.input_node
        command_node_list = parsed_program.node_list
        parsed_program.answer_node
        stack_trace = []

        if not program_ends:
            # After one attempt at completing program, give up checking for affordance
            return program

        previous_input = input_node.text

        while i < len(command_node_list) and command_node_list[i].command_name != "[EOQ]":
            node = command_node_list[i]
            command_name = node.command_name
            command_input = node.command_input
            command_output = node.command_output

            # Check affordance list to run.
            affordance = self.check_builtin(command=command_name)

            # if affordance and previous_input:
            if affordance:
                affordance_output, affordance_details = affordance(command_input, previous_input)

                if affordance_output:
                    # affordance_output and command_output (i.e. GPT-3) need to be interpolated appropriately.
                    # For Search (concatenate the GPT_3 output with retrieval output)
                    if command_name == "[search]":
                        command_output += " " + affordance_output
                    else:
                        command_output = affordance_output

                if self.rerun:
                    # Run GPT-3 again (technically only if the execution is different)
                    program_prefix, rerun_program = self.rerun_program(
                        prefix, command_node_list[: i + 1], command_output
                    )

                    # Replace nodes i+1 though N (again complication is that this should be done EOC i.e. till the program is fully generated.)
                    new_program = program_prefix + rerun_program
                    new_program = new_program.rsplit("----\n", 1)[1].split("\n", 1)[1]
                    new_program = self.complete_program(prefix, new_program)

                    try:
                        parsed_rerun_program = parse_program(new_program)
                    except:
                        # Everytime the rerun fails to generate a legitimate program, its unsafe to assume that it could replace the rest of the original program.
                        # Best course of action is to exit with the best new_program
                        program = new_program
                        break

                    parsed_program = parsed_rerun_program
                    program = new_program
                    command_node_list = (
                        command_node_list[: i + 1] + parsed_rerun_program.node_list[i + 1 :]
                    )
                    parsed_rerun_program.answer_node
                    stack_trace.append(
                        StacktraceItem(
                            node,
                            affordance,
                            affordance_output,
                            affordance_details,
                            new_program,
                            parsed_rerun_program,
                        )
                    )
                else:
                    # Run GPT-3 again (technically only if the execution is different)
                    # program_prefix, rerun_program = self.rerun_program(prefix, command_node_list[:i+1], command_output)
                    prev_command_list = command_node_list[: i + 1]
                    program_prefix = prefix + "\n".join(
                        [
                            Command.convert_to_nlprogram(j + 1, command)
                            for j, command in enumerate(prev_command_list[:-1])
                        ]
                    )
                    # TODO: Add feature for multiline
                    output_sep = "\n" if "\n" in command_output else " "
                    program_sep = "" if len(prev_command_list) == 1 else "\n"
                    program_prefix += (
                        program_sep
                        + Command.convert_to_nlprogram(
                            len(prev_command_list),
                            prev_command_list[-1],
                            input_only=True,
                        )
                        + "\n#{0}:{1}{2}\n".format(
                            len(prev_command_list), output_sep, command_output.strip()
                        )
                    )
                    program_prefix += "\n".join(
                        [
                            Command.convert_to_nlprogram(j + len(prev_command_list) + 1, command)
                            for j, command in enumerate(command_node_list[i + 1 :])
                        ]
                    )
                    new_program = program_prefix
                    # parsed program does not change since the future nodes are not updated.
                    # However, final answer can change due to selective replacement of the intermediate steps.
                    continuation = self.rerun_answer(program_prefix)
                    new_program += continuation
                    new_program = new_program.rsplit("----\n", 1)[1].split("\n", 1)[1]
                    stack_trace.append(
                        StacktraceItem(
                            node,
                            affordance,
                            affordance_output,
                            affordance_details,
                            new_program,
                            parsed_program,
                        )
                    )

            # Output of the current command becomes the input for the next round.
            previous_input = command_output
            i += 1
        # Since run is complete, The final answer in parsed_rerun_program is returned
        # Selectively, we can also return the entire program
        self.execution_details.append(stack_trace)

        return program
